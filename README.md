# Text-Summarization
Text summarization with BERT, GPT and T5 Models

Dataset of a combination of TLDRs written by human experts and author-written TLDRs of computer science papers from OpenReview (SciTLDR)

Comparing different text summarization methods with different models

Models used: T5, Bert, GPT-2
approaches: Cleansed Data, Non-Cleansed Data

Conclusion:
T5-Small excelled in generating coherent and semantically rich summaries, demonstrating its robustness and adaptability in handling diverse input data. 
BERTâ€™s exceptional performance in capturing key details and ensuring precision underscores the strength of its encoder-based architecture
GPT-2 showed some promise as a summarization model, its performance was less consistent compared to T5-Small and BERT. 
